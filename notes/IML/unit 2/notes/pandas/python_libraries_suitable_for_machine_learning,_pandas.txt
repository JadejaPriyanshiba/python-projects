-------------------- TOPICS -----------------
0.) Introduction
1.) Series: Series()Dataframes: DataFrames()
2.) Read CSV File: read_csv()
3.) Cleaning Empty Cells: dropna()
4.) Cleaning Wrong Data: drop()
5.) Removing Duplicates: duplicated()
6.) Pandas Plotting: plot()
7.) Summary And Revision
1.) Introduction
Introduction to Machine Learning (ML)
- Machine Learning is a branch of artificial intelligence where computer systems learn from data to identify patterns and make decisions or predictions, without being explicitly programmed for every specific task.
- For instance, if you want to predict whether a customer will buy a product, instead of coding 'if-then' rules, an ML model learns from historical customer data (age, income, past purchases) to forecast future behavior.

The Crucial Role of Data
- Data is the fuel for machine learning. The quality and preparation of this data directly impact the performance of any ML model.
- Raw data is rarely in a perfect state; it often contains errors, missing pieces, or inconsistencies that must be addressed before analysis.

Python: The ML Ecosystem
- Python has become the go-to language for machine learning due to its simplicity, readability, and extensive collection of specialized libraries. These libraries streamline complex data science and ML tasks.

Introducing Pandas for Data Preparation in ML
- Pandas is a cornerstone Python library specifically engineered for efficient data manipulation and analysis, particularly with structured data.
- In the context of ML, Pandas serves as your primary tool for data preprocessing â€“ the essential steps taken to transform raw data into a clean, usable format for machine learning algorithms.
- Think of Pandas as a highly specialized digital workbench that helps you organize, inspect, and refine your datasets.

Key Contributions of Pandas to ML Workflow (High-Level)
- Data Acquisition: It provides robust tools to load data from various file formats, such as CSV files, Excel spreadsheets, or databases, into a standardized, table-like structure.
- Initial Data Exploration: Allows quick inspection of data, including viewing the first few rows, checking data types, and generating descriptive statistics to understand the dataset's characteristics.
- Data Cleaning and Preprocessing: Facilitates handling common data quality issues. This involves managing missing information, correcting erroneous or inconsistent entries, and identifying or removing duplicate records to ensure data integrity.
- Data Transformation: Supports reshaping and combining datasets, or creating new features from existing ones, which is vital for feature engineering in ML.

Real-world Scenario: Analyzing Customer Feedback
- Imagine you're building an ML model to categorize customer feedback as positive, negative, or neutral.
- Your initial data might be a spreadsheet with comments and perhaps a rating. Pandas would be used to load this data, ensure all ratings are valid, remove blank comments, and check for duplicate feedback entries from the same customer before the text analysis ML model begins its work.

Summary of Key Points
- Machine Learning empowers systems to learn from data to make intelligent decisions.
- Data preparation, starting with raw data and refining it, is a foundational step in ML.
- Pandas is a crucial Python library for this initial phase, designed for managing and cleaning structured data.
- It enables efficient data loading, basic exploration, cleaning of issues like missing values or duplicates, and essential transformations, paving the way for effective machine learning.


2.) Series: Series()Dataframes: DataFrames()
Pandas is a fundamental Python library for data manipulation and analysis, crucial for machine learning. Its core structures, Series and DataFrame, are how we organize and work with data.

1.  Series: A 1-Dimensional Labeled Array

    -   Definition: A Pandas Series is a one-dimensional array-like object capable of holding any data type (integers, strings, floats, Python objects, etc.). It's essentially a column of data.
    -   Key Characteristic: Each element in a Series has a unique label, called an index, which can be implicitly numerical (0, 1, 2...) or explicitly defined (e.g., names, dates).
    -   Analogy: Think of it as a single column in a spreadsheet or a labeled list. If you have a list of student ages, [20, 21, 19], a Series would store these ages, allowing you to access them by their position or a custom label.
    -   Creation: You can create a Series from a list, a NumPy array, or a dictionary.
    -   Example:
        -   import pandas as pd
        -   ages = pd.Series([20, 21, 19], index=['Alice', 'Bob', 'Charlie'])
        -   This Series would store age values with corresponding student names as labels.
    -   Real-world relevance in ML: A Series can represent a single feature (e.g., 'Sepal Length' for all flowers) or a target variable (e.g., 'Species') for your machine learning model. It's often a vector of observations.

2.  DataFrame: A 2-Dimensional Labeled Data Structure

    -   Definition: A Pandas DataFrame is a two-dimensional, size-mutable, tabular data structure with labeled axes (rows and columns). It's essentially a collection of Series objects sharing the same index.
    -   Key Characteristics: DataFrames are like tables, comprising rows and columns. Each column can have a different data type. Both rows and columns have labels (indices for rows, column names for columns).
    -   Analogy: Imagine an entire spreadsheet or a SQL table. Each column in the DataFrame is a Series.
    -   Creation: DataFrames can be created from dictionaries, lists of dictionaries, NumPy arrays, or by reading structured data files like CSVs (using `read_csv()`, a topic you'll cover later).
    -   Example:
        -   data = {'Name': ['Alice', 'Bob'], 'Age': [20, 21], 'GPA': [3.5, 3.8]}
        -   students_df = pd.DataFrame(data)
        -   This creates a table with 'Name', 'Age', and 'GPA' as columns, and two rows of student data.
    -   Real-world relevance in ML: DataFrames are the primary way to store and manipulate entire datasets for machine learning. Your training data, which includes multiple features and possibly a target variable, will typically be structured as a DataFrame.

3.  Relationship Between Series and DataFrame

    -   A DataFrame is essentially a dictionary-like collection of Series objects where keys are column names and values are Series themselves.
    -   When you select a single column from a DataFrame, the result is a Series. For example, selecting the 'Age' column from `students_df` would return an 'Age' Series.

4.  Why are they important for Machine Learning?

    -   Structured Data: They provide an intuitive and efficient way to store, organize, and represent the tabular data that most machine learning algorithms work with.
    -   Data Manipulation: Pandas offers powerful tools for data cleaning, transformation, and preparation tasks (like handling missing values with `dropna()`, removing data with `drop()`, or dealing with duplicates using `duplicated()`, which you'll explore later).
    -   Integration: DataFrames and Series integrate seamlessly with other ML libraries like Scikit-learn, providing the expected input formats for models.

Summary of Key Points:
-   A Series is a one-dimensional, labeled array, representing a single column of data.
-   A DataFrame is a two-dimensional, labeled data structure, like a table, composed of multiple Series (columns).
-   They are fundamental for storing and manipulating datasets in a structured way for machine learning tasks.


3.) Read CSV File: read_csv()
In Machine Learning, acquiring and preparing data is fundamental. CSV (Comma Separated Values) files are a ubiquitous format for storing tabular data. Pandas, a key Python library for data manipulation in ML, provides the pd.read_csv() function, your primary tool for ingesting such data.

This function efficiently reads data from a CSV file and transforms it into a Pandas DataFrame, a structured format ideal for subsequent data analysis and model training.

- What is pd.read_csv()?
  - Its core purpose is to load tabular data from a CSV file directly into a Pandas DataFrame.
  - Basic syntax: data = pd.read_csv('your_file.csv'). It assumes a comma as the separator and the first row as column headers by default.

- Essential Parameters for Real-World Scenarios
  Real-world datasets often deviate from simple CSV formats. Understanding these parameters is crucial for correct data loading, preventing errors that impact your ML workflow.

  1.  filepath_or_buffer: The required path to your CSV file, whether local or a URL.
      - Example: pd.read_csv('datasets/customer_data.csv')

  2.  sep (or delimiter): Specifies the character separating values in the file. Defaults to a comma (',').
      - Use for semicolon-separated (';') or tab-separated ('\t') files.
      - Example: pd.read_csv('metrics.txt', sep='\t')

  3.  header: Indicates which row contains column names (0-indexed). Default is 0. Use None if no header row exists.
      - Example: pd.read_csv('features.csv', header=None)

  4.  names: A list of strings to use as column names, especially useful when header=None.
      - Example: pd.read_csv('data.csv', header=None, names=['id', 'value', 'label'])

  5.  index_col: Specifies which column should be used as the DataFrame's index.
      - Example: pd.read_csv('users.csv', index_col='UserID')

  6.  skiprows: Skips the specified number of initial rows or a list of row numbers. Useful for ignoring metadata.
      - Example: pd.read_csv('sensor_logs.csv', skiprows=5) to skip the first 5 rows.

  7.  nrows: Reads only a specified number of rows from the beginning of the file. Ideal for quick previews of large datasets.
      - Example: pd.read_csv('big_data.csv', nrows=1000)

  8.  encoding: Specifies the character encoding of the file (e.g., 'utf-8', 'latin-1'). Essential for files with special characters or different regional encodings.
      - Example: pd.read_csv('international_names.csv', encoding='latin-1')

- Real-World Significance for Machine Learning
  - Correctly loading data is the indispensable first step. Incorrect parameter usage (e.g., wrong 'sep') will lead to a malformed DataFrame, hindering all subsequent data cleaning and ML model development.
  - Parameters like 'nrows' can significantly speed up initial data exploration on massive datasets, allowing faster iterative development.

- Summary of Key Points
  - pd.read_csv() is vital for loading external CSV data into Pandas DataFrames.
  - Its versatile parameters enable handling various CSV formats and optimizing data ingestion.
  - Mastering these parameters ensures a robust foundation for any Machine Learning project.


4.) Cleaning Empty Cells: dropna()
Missing data, often represented as "NaN" (Not a Number) in Pandas DataFrames, is a common challenge in real-world datasets for Machine Learning. Machine Learning algorithms typically require complete data and cannot directly process these empty cells, leading to errors or biased model training. Pandas provides robust tools to handle such situations efficiently.

-   Understanding Empty Cells in ML Context
    Empty cells signify missing observations or information. In ML, ignoring them can lead to faulty predictions or an inability for algorithms to run at all. Pandas helps us identify and manage these gaps effectively.

-   Introducing dropna(): Your First Line of Defense
    The `dropna()` method in Pandas is a straightforward way to remove rows or columns that contain missing values (NaNs). It's a quick and essential step for data cleaning before model training.

-   Basic Usage: Removing Rows with Any Missing Data
    By default, `dropna()` removes any row that contains at least one NaN.
    Example: `df.dropna()`
    This returns a new DataFrame with missing rows excluded. The original DataFrame `df` remains unchanged unless specified.

-   Controlling Removal Scope: `axis` Parameter
    -   `axis=0` (default): Removes rows with missing values.
    -   `axis=1`: Removes columns with missing values.
    Example: `df.dropna(axis=1)` would remove columns containing NaNs.

-   Refining Removal Strategy: `how` Parameter
    -   `how='any'` (default): Drop a row/column if it has *any* NaN values.
    -   `how='all'`: Drop a row/column only if *all* its values are NaN. This is useful for entirely empty records.
    Example: `df.dropna(how='all')`

-   Setting a Threshold for Non-Missing Data: `thresh` Parameter
    The `thresh` parameter specifies the minimum number of non-NaN values required for a row/column to be kept. If a row/column has fewer non-NaN values than `thresh`, it is dropped.
    Example: `df.dropna(thresh=3)` keeps only rows with at least 3 valid entries.

-   Targeting Specific Columns for Missing Data Check: `subset` Parameter
    Sometimes you only care about missing values in particular columns. The `subset` parameter allows you to specify a list of column labels to consider when looking for NaNs.
    Example: `df.dropna(subset=['Age', 'Salary'])` will only drop rows where 'Age' or 'Salary' is NaN.

-   Modifying in Place: `inplace` Parameter
    By default, `dropna()` returns a new DataFrame. To modify the original DataFrame directly, use `inplace=True`.
    Example: `df.dropna(inplace=True)`

-   Real-World Significance and Considerations
    Using `dropna()` is crucial for preparing clean datasets for ML algorithms like linear regression or decision trees. However, simply dropping data can lead to significant loss of information, especially in smaller datasets. It's a trade-off: cleaning for accuracy versus retaining data for sufficiency. Sometimes, instead of dropping, we might fill empty cells with estimated values (imputation), which is a more advanced technique.

Summary of Key Points:
-   `dropna()` removes rows or columns with missing (NaN) data.
-   `axis` controls whether to drop rows (0) or columns (1).
-   `how` defines the condition: `any` (default) or `all` NaNs.
-   `thresh` sets a minimum count of valid data points to keep.
-   `subset` targets specific columns for NaN detection.
-   `inplace=True` modifies the original DataFrame directly.
-   It's a foundational data cleaning step for ML, but consider data loss.


5.) Cleaning Wrong Data: drop()
Cleaning Wrong Data: drop()

1. Understanding Wrong Data in Machine Learning
- In machine learning, "wrong data" refers to entries that are factually incorrect, inconsistent, or lie outside expected ranges, making them invalid for analysis. Unlike empty cells (missing data) or duplicates, wrong data actively misrepresents reality and can mislead our algorithms.
- This type of data often arises from human error during data entry, faulty sensors, or incorrect data transformations.
- Examples include a person's age being 200 (impossible), a temperature reading of -1000 degrees Celsius (physically impossible), or a categorical column like "Gender" having an entry "Xyz" (not a valid category).
- Such data points can severely skew model training, leading to inaccurate predictions, biased models, and poor generalization to new data. Identifying and removing them is a crucial step in data preprocessing to ensure data quality.

2. The Role of Pandas drop() for Wrong Data
- Once wrong data has been identified, for instance, by checking data against domain-specific rules or using statistical outlier detection, the Pandas drop() method becomes essential.
- While dropna() is specifically used for handling missing values (empty cells), drop() is a versatile tool for removing specific rows or columns that have been explicitly flagged as problematic or irrelevant due to containing wrong data.
- It provides a granular way to cleanse your DataFrame, allowing for precise removal based on index labels (for rows) or column names (for columns).

3. Using drop() Parameters for Wrong Data Removal
- To effectively remove wrong data, drop() utilizes key parameters:
- labels: This is the primary argument to specify exactly what you want to remove. It can be a single index label, a single column name, or a list of multiple labels/names if you need to remove several specific entries or columns.
- axis: This parameter determines the direction of the removal.
- axis=0 (default): To drop rows. You provide the index labels of the rows you want to eliminate. This is common when individual observations are incorrect.
- axis=1: To drop columns. You provide the names of the columns you want to eliminate. This is used if an entire feature (column) contains consistently wrong or unusable data.
- inplace: A boolean (True/False) that dictates whether the operation modifies the original DataFrame directly (True) or returns a new DataFrame with the specified rows/columns removed (False, default). For permanent changes to your working DataFrame, inplace=True is frequently used, but be cautious as it modifies the DataFrame permanently.

4. Real-World Example Scenario
- Consider a dataset of patient health records where some "Body_Temperature" readings are found to be below 20 degrees Celsius or above 45 degrees Celsius (values physiologically impossible for a living human).
- First, you would identify these outlier rows. For example, by filtering: wrong_temp_indices = df[(df['Body_Temperature'] < 20) | (df['Body_Temperature'] > 45)].index.
- Then, you'd use df.drop(labels=wrong_temp_indices, axis=0, inplace=True) to remove these specific, incorrect patient entries.
- Another case: if a column named "Legacy_Reference_Code" was found to contain highly inconsistent, outdated, and non-standardized alphanumeric strings (making it "wrong" for numerical analysis or categorization), you might drop the entire column using df.drop(labels='Legacy_Reference_Code', axis=1, inplace=True).

5. Summary of Key Points
- Wrong data refers to factually incorrect, inconsistent, or impossible values that can severely compromise machine learning model training.
- The Pandas drop() method is crucial for precisely removing these identified problematic rows or columns.
- Its main parameters are labels (what to remove by index or name), axis (0 for rows, 1 for columns), and inplace (to modify the original DataFrame).
- Effective use of drop() for wrong data relies on prior data validation and domain knowledge to identify the incorrect entries or features.


6.) Removing Duplicates: duplicated()
Introduction to Removing Duplicates: duplicated()

In data cleaning, a vital step for machine learning, identifying and managing duplicate entries is crucial. Duplicates can lead to biased models and inefficient training. The Pandas duplicated() method helps us find these repeated rows effectively.

1. What is duplicated()?
The duplicated() method of a Pandas DataFrame identifies identical rows. It returns a Boolean Series: True for a duplicate row, False for a unique row (or the first occurrence of a set of duplicates).

2. How duplicated() works (Basic usage)
By default, df.duplicated() compares each row to preceding ones. If an exact match across all columns is found, that row is marked True. The very first instance of any unique row combination is always False.
Example:
Data: (101, Laptop), (102, Mouse), (101, Laptop), (103, Keyboard)
df.duplicated() output: False, False, True, False

3. Key Parameters for identifying duplicates
- subset: A list of column names. Duplication is checked only within these specific columns.
  - Real-world example: In an 'orders' dataset, you might define a duplicate row only if 'customer_id' AND 'order_id' match, ignoring other columns like 'timestamp' if slight differences are expected.

- keep: Determines which occurrence(s) to mark as False (i.e., not a duplicate).
  - 'first' (default): Marks all duplicates as True except the first.
  - 'last': Marks all duplicates as True except the last.
  - False: Marks all occurrences within a duplicate set as True. Useful to see every single repeated row.

4. Why it matters for Machine Learning
Duplicate data negatively impacts ML models by:
- Creating Bias: Over-representing certain patterns can skew model learning.
- Causing Overfitting: Models might memorize redundant data instead of learning general rules.
- Reducing Efficiency: Wastes computation during training.
- Compromising Data Integrity: Leads to an inaccurate representation of the true data distribution.

5. Removing Duplicates with drop_duplicates()
While duplicated() identifies, the drop_duplicates() method physically removes them. It uses similar 'subset' and 'keep' logic.
- To remove all identified duplicates, keeping the first:
  df_cleaned = df.drop_duplicates()
- To remove based on 'customer_id' and 'product', keeping the last:
  df_cleaned = df.drop_duplicates(subset=['customer_id', 'product'], keep='last')

Summary of Key Points:
- duplicated() identifies duplicate rows, returning a Boolean Series.
- 'subset' and 'keep' parameters customize duplicate definition and flagging.
- Removing duplicates is vital for robust, unbiased, and efficient ML models.
- drop_duplicates() is used for the actual removal of identified duplicates.


7.) Pandas Plotting: plot()
Pandas Plotting: plot()

After collecting and cleaning your data using Pandas DataFrames, the next critical step in Machine Learning is Exploratory Data Analysis (EDA). This involves visualizing your data to understand its patterns, distributions, and relationships before building models. Pandas simplifies this by integrating with Matplotlib, allowing you to create various plots directly from DataFrames and Series using the `.plot()` method.

The plot() Method:
-   The `.plot()` method is a convenient wrapper around Matplotlib's plotting functionalities.
-   It is called directly on a DataFrame or a Series object.
-   Its primary parameter, `kind`, specifies the type of plot you want to generate. The default `kind` is 'line'.

Common Plot Types and Their ML Relevance:

1- Line Plot (kind='line')
-   Purpose: Visualizes trends over a continuous range, typically time or an ordered index.
-   ML Relevance: Essential for time-series analysis (e.g., sensor readings, stock prices) to observe data evolution.
-   Example: `df['temperature_readings'].plot(kind='line')`

2- Bar Plot (kind='bar')
-   Purpose: Compares discrete categories. Often used for counts or aggregated values.
-   ML Relevance: Useful for showing the frequency of categorical features, class imbalance in target variables, or feature importance.
-   Example: `df['product_category'].value_counts().plot(kind='bar')`

3- Scatter Plot (kind='scatter')
-   Purpose: Shows the relationship between two numerical variables (x and y axes).
-   ML Relevance: Helps identify correlations, clusters, or outliers between features, which is crucial for feature selection or understanding feature interactions.
-   Example: `df.plot(kind='scatter', x='feature_1', y='feature_2')`

4- Histogram (kind='hist')
-   Purpose: Displays the distribution of a single numerical variable by showing the frequency of values within specified 'bins'.
-   ML Relevance: Fundamental for understanding a feature's distribution (e.g., normal, skewed), identifying modes, and detecting outliers. It informs data preprocessing steps like standardization or normalization.
-   Example: `df['age'].plot(kind='hist', bins=20)`

5- Box Plot (kind='box')
-   Purpose: Summarizes the distribution of a numerical variable using quartiles, median, and potential outliers.
-   ML Relevance: Excellent for comparing distributions across different groups or quickly identifying extreme values (outliers) that might need special handling in your ML pipeline.
-   Example: `df.plot(kind='box', y='numerical_feature')`

Basic Customization:
You can easily add titles, axis labels, and adjust plot size:
-   `df.plot(kind='hist', title='Distribution of Age', xlabel='Age (Years)', ylabel='Frequency', figsize=(8, 5))`
Many other Matplotlib parameters can be passed directly to customize the plots further.

Real-World Understanding and ML Connection:
In a real-world machine learning project, visualization with `df.plot()` is indispensable. It's how data scientists gain initial insights, validate data cleaning efforts, discover hidden patterns, and communicate findings. For instance, a histogram might reveal that your target variable is heavily skewed, suggesting a transformation might improve model performance. A scatter plot could show a strong linear relationship between two features, informing feature engineering.

Key Points Summary:
-   Pandas `plot()` method provides an easy way to visualize DataFrames and Series.
-   It's a wrapper for Matplotlib, simplifying data visualization for EDA.
-   The `kind` parameter specifies plot types like 'line', 'bar', 'scatter', 'hist', 'box'.
-   Each plot type serves a specific purpose in understanding data trends, distributions, and relationships.
-   Visualization is a fundamental step in Machine Learning, aiding in data understanding, outlier detection, and feature engineering.


8.) Summary And Revision
Understanding "Summary And Revision" in the context of Pandas for Machine Learning is a crucial final step in data preparation, ensuring your data is clean, well-understood, and ready for model training. It acts as a quality assurance phase.

1- Understanding Summary and Revision in Data Preparation
 This stage involves a thorough review of the processed dataset. Summarization means generating statistical and structural overviews of the data. Revision involves making final adjustments or re-evaluating previous cleaning decisions based on these summaries.

2- Why is this stage crucial for Machine Learning?
 - Model Performance: The quality of your data directly impacts your machine learning model's performance. Poorly summarized or unrevised data can lead to inaccurate predictions or biased models.
 - Data Insight: It provides deep insights into the data's characteristics, distributions, and potential relationships, which guides feature engineering and model selection.
 - Anomaly Detection: Often reveals outliers, inconsistencies, or patterns that might have been overlooked during initial data cleaning.
 - Informed Decision Making: Helps you confirm if your data truly represents the problem you are trying to solve.

3- Pandas Tools for Data Summarization
 Pandas provides powerful functions to quickly summarize your DataFrame:
 - .describe(): This function generates descriptive statistics for numerical columns. It shows count, mean, standard deviation, min/max values, and quartiles (25%, 50%, 75%).
   - Example: After cleaning customer age and income data, .describe() helps understand the average age, income spread, and potential outlier values like very young or old customers.
 - .info(): This method prints a concise summary of a DataFrame, including the index dtype and column dtypes, non-null values, and memory usage. It's vital for confirming data types and detecting remaining missing values.
   - Example: Running .info() on a product dataset confirms all product IDs are non-null and correctly identified as integers, and that product descriptions are string types.
 - .value_counts(): Primarily used on Series (a single column), it returns a Series containing counts of unique values. It's perfect for understanding the distribution of categorical features.
   - Example: Using df['City'].value_counts() on a customer dataset shows how many customers are from each city, helping identify dominant regions.

4- Pandas for Data Revision and Refinement
 Based on the summaries, you might need to revise your data:
 - Conditional Selection and Filtering: You might use boolean indexing to re-examine specific subsets of data. For instance, if .describe() shows a very high maximum value for a 'transaction amount' column, you can filter for those records to check their legitimacy.
 - Visual Checks (using .plot()): While we've covered plotting, it's essential for revision. Histograms can show distributions, scatter plots can reveal relationships or clusters. These visual aids quickly confirm or contradict statistical summaries.
   - Example: A histogram of 'customer ages' might reveal an unexpected bimodal distribution, prompting a revision to investigate why two distinct age groups are present.
 - Re-evaluating Cleaning Decisions: Sometimes, summarization highlights that a previous cleaning step, like dropping rows with specific values, might have been too aggressive or not thorough enough. You might revisit the original data or adjust your cleaning logic.

5- Real-World Application
 Imagine you're preparing a dataset of patient health records for a diagnostic model. After cleaning, .info() confirms no missing values in critical columns like 'blood pressure'. .describe() gives insights into the average blood pressure and cholesterol levels, highlighting if any values are outside normal ranges. .value_counts() on 'gender' confirms the proportion of male/female patients. If a cholesterol value appears unusually high, you'd revise it by checking the original source or marking it as an outlier if confirmed. This iterative summary and revision improves the dataset's integrity, leading to a more reliable diagnostic model.

Key Takeaways:
 - Summary and Revision are the final, critical checks for your machine learning dataset.
 - Pandas provides essential functions like .describe(), .info(), and .value_counts() for effective summarization.
 - This stage involves both statistical overview and visual inspection to ensure data quality.
 - It directly impacts the accuracy, reliability, and fairness of your machine learning models.


