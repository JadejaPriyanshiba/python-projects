Topic: Chatbots

1. What is the primary purpose of a rule-based chatbot?
(a) To generate human-like responses using large language models.
(b) To follow predefined scripts and keywords for interactions.
(c) To learn from user conversations without explicit programming.
(d) To provide visual recommendations based on user sentiment.
2. Explain the core difference between a generative chatbot (like ChatGPT) and a retrieval-based chatbot in terms of how they formulate responses.
3. Which component is primarily responsible for converting spoken user input into text for a digital voice assistant?
(a) Natural Language Understanding (NLU)
(b) Dialogue Manager
(c) Speech Recognition (ASR)
(d) Text-to-Speech (TTS)
4. A company wants to implement a chatbot for customer support. Describe two key challenges they might face when using a purely rule-based approach for handling complex customer queries, and suggest how a more advanced AI approach could mitigate one of these challenges.
5. Briefly describe the function of an intent classifier within a typical chatbot's Natural Language Understanding (NLU) module. What kind of machine learning model might be used for this task?
6. The Transformer architecture, fundamental to models like ChatGPT, introduced the concept of "attention mechanisms." What is the primary benefit of the self-attention mechanism in processing input sequences?
(a) It reduces the computational cost of processing long sequences by using fixed-size windows.
(b) It allows the model to weigh the importance of different words in the input sequence relative to each other.
(c) It enables the model to translate text directly from one language to another without intermediate steps.
(d) It enforces a strict left-to-right processing order for all tokens in the sequence.
7. Compare and contrast the capabilities of a context-aware chatbot with a stateless chatbot. Provide a simple example where context awareness is crucial.
8. Outline a high-level architectural design for a chatbot system that integrates with an external knowledge base (e.g., a product catalog) to answer specific user queries. Mention the main components and their interactions.
9. Discuss one potential ethical concern related to the deployment of large language model chatbots like ChatGPT in critical applications (e.g., medical advice, legal consultation).
10. Explain the concept of "few-shot learning" or "in-context learning" as it applies to large language models. How does this capability reduce the need for extensive fine-tuning?
11. Consider a simple chatbot designed to respond to "hello" with "Hi there!" and "how are you" with "I'm doing well, thank you." Write a basic Python pseudocode snippet that demonstrates how to implement this using conditional statements (if/elif/else) for pattern matching.
12. A chatbot is being developed for a smart home system. When a user says "Turn on the lights in the living room," the chatbot needs to identify the 'intent' (turn_on), the 'device' (lights), and the 'location' (living_room). What NLP task is being performed to extract 'device' and 'location' from the utterance?
13. How do digital voice assistants handle ambiguity in user commands (e.g., "Play some music" without specifying genre or artist)? Briefly describe one strategy.
14. Name two distinct metrics commonly used to evaluate the performance of a chatbot in a customer service scenario, and briefly explain what each metric measures.
15. A travel agency wants to build a chatbot to help users book flights. Besides natural language understanding, what other AI component, often associated with recommendation systems, could significantly enhance the user experience by suggesting destinations or flight times? Explain how.


Topic: ChatGPT

Section: Multiple Choice Questions
16.
The foundational architecture behind ChatGPT is primarily based on which of the following neural network models?
(a) Recurrent Neural Network (RNN)
(b) Long Short-Term Memory (LSTM)
(c) Transformer
(d) Convolutional Neural Network (CNN)
17.
Which of the following best describes the type of data primarily used for the pre-training phase of Large Language Models like ChatGPT?
(a) Structured relational databases
(b) High-resolution image datasets
(c) Vast amounts of diverse text and code from the internet
(d) Time-series sensor data
18.
The attention mechanism, a core component of the Transformer architecture, primarily addresses which challenge in processing sequences for models like ChatGPT?
(a) Reducing the number of layers required for deep learning
(b) Enabling the model to weigh the importance of different words in a sequence when generating new ones
(c) Accelerating the speed of gradient descent optimization
(d) Converting continuous input data into discrete tokens
19.
Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in the development of ChatGPT. What is its primary purpose?
(a) To generate the initial pre-training dataset from human interactions
(b) To align the model's outputs with human preferences and instructions
(c) To reduce the computational cost of model inference
(d) To automatically translate text into different languages
20.
Unlike a rule-based traditional chatbot, ChatGPT's ability to generate coherent and contextually relevant responses stems primarily from its:
(a) Pre-defined scripts and decision trees
(b) Large language model architecture and extensive pre-training
(c) Dependence on explicit if-then-else conditions
(d) Direct access to real-time external databases for every query
21.
A significant ethical challenge associated with generative AI models like ChatGPT is "hallucination." Which of the following best defines this phenomenon?
(a) The model's inability to understand complex queries
(b) The model generating outputs that are factually incorrect or nonsensical, despite sounding plausible
(c) The model accidentally deleting user data during conversation
(d) The model requiring visual input to process textual queries
22.
"Prompt engineering" is a critical skill for effectively interacting with ChatGPT and similar LLMs. What does it primarily involve?
(a) Modifying the model's internal weights and biases
(b) Designing and refining input queries to elicit desired model responses
(c) Developing new pre-training datasets for the model
(d) Optimizing the computational resources for model deployment
23.
A computer engineering student wishes to integrate ChatGPT's capabilities into a custom web application. Which method is most commonly used for programmatic interaction with the model?
(a) Directly modifying the model's source code
(b) Training a separate, smaller model to mimic ChatGPT's behavior
(c) Utilizing an Application Programming Interface (API) provided by the model developer
(d) Sending direct database queries to the model's underlying data store
24.
The concept of "transfer learning" is central to how models like ChatGPT are adapted for specific tasks. Which process exemplifies transfer learning in this context?
(a) Training a model from scratch on a completely new dataset for every task
(b) Using a pre-trained general-purpose model and fine-tuning it on a smaller, task-specific dataset
(c) Only using a model for tasks it was explicitly designed for during initial development
(d) Implementing a recommendation algorithm to suggest optimal training parameters
25.
Training large language models like ChatGPT requires substantial computational resources. Which hardware component is most critical for efficiently processing the massive parallel computations involved in training these models?
(a) Central Processing Unit (CPU)
(b) Hard Disk Drive (HDD)
(c) Graphics Processing Unit (GPU)
(d) Random Access Memory (RAM)
26.
Consider a scenario where a company wants to provide personalized product recommendations to users and also offer a natural language interface for customer support. Which statement accurately differentiates the typical AI approach for these two tasks?
(a) Both tasks would primarily use the same ChatGPT-like generative model.
(b) Product recommendations would likely use a collaborative filtering or content-based recommendation algorithm, while the customer support interface could leverage a model like ChatGPT.
(c) ChatGPT is ideal for recommendations, and traditional rule-based systems are better for customer support.
(d) Virtual face filters provide a better solution for both personalized recommendations and customer support.
27.
While highly capable, ChatGPT and similar LLMs still face challenges in maintaining long-term memory or consistent persona across extended conversations. This limitation is primarily due to:
(a) The inherent inability of neural networks to store information
(b) The model's architecture processing fixed-size context windows, requiring external mechanisms for long-term state management
(c) Insufficient training data on long conversations
(d) Hardware limitations preventing the storage of complete conversation histories on the server
28.
A digital voice assistant typically integrates various AI components. How might a model like ChatGPT enhance the capabilities of a modern digital voice assistant?
(a) By replacing the speech-to-text and text-to-speech modules entirely
(b) By providing more nuanced and contextually aware natural language understanding and response generation
(c) By solely handling the alarm setting and calendar management functions
(d) By primarily improving the accuracy of facial recognition for user authentication
29.
When evaluating the performance of Large Language Models like ChatGPT for tasks such as text generation or summarization, which of the following metrics is often employed alongside human evaluation to assess linguistic quality and similarity?
(a) Mean Squared Error (MSE)
(b) Intersection over Union (IoU)
(c) Bilingual Evaluation Understudy (BLEU) score
(d) F1-score
30.
The process by which an input text sequence is converted into numerical representations (vectors) that the Transformer model can process is known as:
(a) Convolutional filtering
(b) Vector quantization
(c) Tokenization and embedding
(d) Recurrent encoding


Topic: Recommendation Algorithm

Section: Multiple Choice Questions
16. Which of the following is the primary goal of a recommendation algorithm?
(a) To categorize items into predefined classes.
(b) To predict user preferences and suggest relevant items.
(c) To compress large datasets for efficient storage.
(d) To analyze the sentiment of user reviews.
17. In the context of recommendation systems, what does the "cold-start problem" primarily refer to?
(a) The difficulty in recommending items that are unpopular.
(b) The challenge of recommending items to new users or new items to any user due to lack of historical data.
(c) The issue of slow system response times during peak usage.
(d) The inability to handle dynamic changes in user preferences over time.
18. A content-based recommendation system would primarily rely on which of the following to suggest a new movie to a user?
(a) Movies watched by users similar to the current user.
(b) Features of movies the user has previously liked (e.g., genre, actors, director).
(c) The average rating of all movies in the database.
(d) Real-time trends of movie popularity across all users.
19. Which of the following techniques is most commonly associated with collaborative filtering that leverages latent factors?
(a) Apriori Algorithm for association rule mining.
(b) Term Frequency-Inverse Document Frequency (TF-IDF).
(c) Singular Value Decomposition (SVD) or Matrix Factorization.
(d) K-Means Clustering for item segmentation.
20. When evaluating a recommendation system, which metric directly measures the difference between predicted and actual ratings on a continuous scale?
(a) Precision@K
(b) Recall@K
(c) Root Mean Squared Error (RMSE)
(d) Coverage
21. Consider a scenario where a digital assistant needs to suggest follow-up questions or actions based on a user's initial query. This functionality shares conceptual similarities with which aspect of recommendation systems?
(a) Item-based collaborative filtering for suggesting similar items.
(b) Content-based filtering using features of the initial query to match relevant follow-ups.
(c) Matrix factorization for uncovering latent conversational topics.
(d) All of the above, depending on the specific implementation strategy.
22. What is the "sparsity problem" in recommendation systems, particularly for collaborative filtering?
(a) The difficulty in storing a large volume of user-item interaction data.
(b) The challenge when the user-item interaction matrix has very few recorded ratings.
(c) The lack of computational resources to process complex recommendation algorithms.
(d) The presence of too many irrelevant features in item descriptions.
23. Which type of recommendation system would be most susceptible to the "filter bubble" phenomenon?
(a) Only hybrid recommendation systems.
(b) Systems that heavily rely on past user preferences without introducing novelty.
(c) Systems that solely recommend based on global popularity.
(d) Systems that use random sampling for recommendations.
24. If you are designing a recommendation system where user privacy is paramount, which approach would you prioritize to minimize direct sharing of individual user data?
(a) User-based collaborative filtering with explicit ratings.
(b) Item-based collaborative filtering with explicit ratings.
(c) Content-based filtering using aggregated item features and user profiles.
(d) Deep learning models requiring extensive personal data.
25. A student is developing a recommendation system using a k-Nearest Neighbors (k-NN) approach. For calculating similarity between users based on their ratings, which of the following would be an appropriate similarity metric?
(a) Manhattan Distance
(b) Euclidean Distance
(c) Cosine Similarity
(d) All of the above, depending on data characteristics and desired behavior.
26. Which of the following is NOT a typical component or phase in the lifecycle of a recommendation system?
(a) Data collection and preprocessing.
(b) Model training and evaluation.
(c) Recommendation generation and serving.
(d) Manual content creation by human experts for every recommendation.
27. When a virtual face filter application suggests new filters to a user based on their past usage and current preferences (e.g., if they frequently use animal filters, suggesting new animal-themed ones), which recommendation approach is most likely being applied?
(a) User-based collaborative filtering
(b) Item-based collaborative filtering
(c) Content-based filtering
(d) Popularity-based filtering
28. The concept of "implicit feedback" in recommendation systems refers to:
(a) User ratings provided on a numerical scale (e.g., 1-5 stars).
(b) User actions that indicate preference without explicit rating, such as clicks, views, or purchases.
(c) Feedback provided by system administrators for quality control.
(d) User feedback obtained through direct surveys or interviews.
29. What is the primary benefit of using a hybrid recommendation system compared to a pure content-based or pure collaborative filtering system?
(a) It is always faster to compute recommendations.
(b) It completely eliminates the cold-start problem.
(c) It can combine the strengths of different approaches to mitigate individual weaknesses (e.g., cold start, sparsity, filter bubble).
(d) It requires significantly less data for training.
30. In a large-scale e-commerce recommendation system, what would be the most significant challenge when scaling a traditional user-based collaborative filtering algorithm?
(a) The lack of sufficiently powerful GPUs for model training.
(b) The computational cost of calculating user-user similarities across millions of users.
(c) The difficulty in obtaining explicit user ratings for all items.
(d) The frequent need to update item descriptions.


Topic: Digital (Voice) Assistant

Section: Multiple Choice Questions
31. Which of the following best describes the primary function of a Digital Voice Assistant?
(a) To generate visual content based on text prompts.
(b) To process spoken language commands and provide relevant responses or actions.
(c) To optimize network traffic for voice over IP calls.
(d) To analyze emotional states from facial expressions.
32. The process within a Digital Voice Assistant that converts spoken words into machine-readable text is known as:
(a) Natural Language Generation (NLG)
(b) Automatic Speech Recognition (ASR)
(c) Text-to-Speech (TTS)
(d) Dialogue Management (DM)
33. Which component of a Digital Voice Assistant is primarily responsible for understanding the user's intention and extracting relevant entities from the converted text?
(a) Automatic Speech Recognition (ASR)
(b) Natural Language Understanding (NLU)
(c) Text-to-Speech (TTS)
(d) Data Storage and Retrieval (DSR)
34. In a typical Digital Voice Assistant architecture, the module that maintains the state of the conversation, tracks user context, and decides the next appropriate action or response is called:
(a) Semantic Parser
(b) Acoustic Model
(c) Dialogue Management System
(d) Emotion Recognition Engine
35. The conversion of a Digital Voice Assistant's generated text response back into audible speech for the user is handled by which technology?
(a) Natural Language Processing (NLP)
(b) Speech Synthesis Markup Language (SSML)
(c) Text-to-Speech (TTS)
(d) Voice Activity Detection (VAD)
36. A significant challenge for Natural Language Understanding (NLU) in Digital Voice Assistants involves:
(a) Generating highly realistic synthesized voices.
(b) Distinguishing between different user accents in Automatic Speech Recognition.
(c) Resolving ambiguities, understanding context, and handling implicit requests.
(d) Efficiently transmitting audio data over varying network conditions.
37. How do Large Language Models (LLMs), like those powering ChatGPT, enhance the capabilities of modern Digital Voice Assistants?
(a) By exclusively improving the speed of Automatic Speech Recognition.
(b) By enabling more natural, coherent, and context-aware conversational responses.
(c) By directly controlling smart home devices without user commands.
(d) By reducing the processing power required for Text-to-Speech synthesis.
38. While both are conversational AI, a key distinction between a traditional chatbot and a Digital Voice Assistant often lies in:
(a) The ability to access external databases.
(b) Their capacity for deep learning.
(c) Their primary interaction modality (text vs. voice).
(d) The use of sentiment analysis.
39. A critical ethical concern associated with the widespread adoption of Digital Voice Assistants is:
(a) The limited variety of voice tones available for TTS.
(b) The potential for continuous recording and privacy breaches.
(c) The complexity of integrating with legacy smart devices.
(d) The high cost of developing new wake words.
40. How might a recommendation algorithm be integrated into a Digital Voice Assistant's functionality?
(a) To suggest relevant products, music, or information based on user preferences and past interactions.
(b) To optimize the audio compression algorithms used in ASR.
(c) To prioritize which wake word to listen for in a multi-assistant environment.
(d) To translate spoken language into different written scripts.
41. Which type of deep learning architecture is widely used in Automatic Speech Recognition (ASR) systems, particularly for capturing sequential dependencies in audio signals?
(a) Convolutional Neural Networks (CNNs) for image feature extraction.
(b) Generative Adversarial Networks (GANs) for synthetic data generation.
(c) Recurrent Neural Networks (RNNs) like LSTMs or GRUs, and more recently Transformers.
(d) Support Vector Machines (SVMs) for traditional classification tasks.
42. In a scenario where multiple people are speaking in a room, which AI technique would be crucial for a Digital Voice Assistant to correctly attribute spoken words to individual speakers?
(a) Voice Activity Detection (VAD)
(b) Speaker Diarization
(c) Pitch Extraction
(d) Emotion Recognition
43. The "wake word" detection mechanism in Digital Voice Assistants presents a significant technical challenge primarily due to:
(a) The need to accurately identify a very short, specific audio pattern while minimizing false positives in noisy environments.
(b) The difficulty in synthesizing a pleasant-sounding confirmation chime.
(c) The computational overhead of displaying a visual confirmation.
(d) The process of uploading user preferences to a cloud server.
44. Which of the following represents a potential security vulnerability specific to Digital Voice Assistants?
(a) Inability to update firmware remotely.
(b) Susceptibility to "dolphin attacks" or inaudible commands.
(c) Limited battery life of the device.
(d) Lack of physical buttons for interaction.
45. Considering the future of AI interaction, how might Digital Voice Assistants conceptually integrate with technologies like Virtual Face Filters in a multimodal AI system?
(a) By providing voice commands to activate or change filters based on spoken preferences.
(b) By using face filter technology to enhance the assistant's microphone sensitivity.
(c) By solely relying on facial expressions to understand user intent, replacing voice input entirely.
(d) By converting facial movements directly into spoken words for TTS.


Topic: Virtual Face Filters

Section: Multiple Choice Questions
46.  Which of the following is the primary computer vision task underlying the initial detection and tracking of a user's face in a virtual face filter application?
(a) Object Classification
(b) Object Segmentation
(c) Facial Landmark Detection
(d) Optical Flow Estimation
47.  For generating highly realistic, novel facial expressions or styles beyond simple overlays (e.g., aging effects or cartoonization), which deep learning architecture is most frequently employed in advanced virtual face filters?
(a) Recurrent Neural Network (RNN)
(b) Support Vector Machine (SVM)
(c) Generative Adversarial Network (GAN)
(d) K-Means Clustering
48.  A significant challenge in developing virtual face filters for real-time mobile applications, especially concerning resource consumption and user experience, is:
(a) Storing large filter asset files on device.
(b) Ensuring cross-platform compatibility across different operating systems.
(c) Minimizing computational latency for frame-by-frame processing.
(d) Managing user data for personalized filter recommendations.
49.  What type of data is most crucial for training machine learning models that accurately identify and localize facial key points (landmarks) across diverse individuals and conditions for virtual face filters?
(a) Large datasets of text descriptions of facial features.
(b) Audio recordings of speech patterns corresponding to facial movements.
(c) Images/videos annotated with precise coordinates of facial key points.
(d) Demographic information and user preferences for filter styles.
50.  From an ethical standpoint, a major concern associated with the widespread use of highly realistic virtual face filters, especially when considering advancements like deepfakes, is:
(a) The excessive battery consumption on mobile devices.
(b) The potential for privacy breaches through facial data collection without explicit consent.
(c) The increased bandwidth usage for filter downloads.
(d) The difficulty in updating filter libraries for new trends.
51.  To ensure a 3D virtual face filter (e.g., a hat or glasses) remains correctly positioned and oriented on a user's head as they move, which capability is essential for the underlying AI system?
(a) Audio-to-text transcription
(b) Gaze tracking and redirection
(c) Head pose estimation and tracking
(d) Voice recognition and command parsing
52.  Virtual face filters are a prime application of Augmented Reality (AR) because they:
(a) Completely replace the real-world view with a virtual environment.
(b) Introduce virtual elements that are rendered and integrated into a live view of the real world.
(c) Focus solely on collecting and analyzing user biometric data.
(d) Create a fully immersive virtual experience using specialized head-mounted displays.
53.  Which programming library or framework is widely recognized and frequently utilized for its robust computer vision functionalities, including real-time image processing and feature detection, making it suitable for virtual face filter development?
(a) Scikit-learn
(b) TensorFlow.js
(c) OpenCV
(d) Pandas
54.  When evaluating the user experience of a real-time virtual face filter, which performance metric is most critical for ensuring smooth and interactive feedback without noticeable delays?
(a) Model Accuracy (how well it detects features)
(b) Frames Per Second (FPS) or Latency
(c) Memory Footprint (RAM usage)
(d) Model Size (disk space)
55.  Consider a scenario where a virtual face filter dynamically changes based on the user's emotional state (e.g., a happy filter for a smile). This advanced functionality would most directly involve the integration of which AI capability?
(a) Natural Language Processing (NLP)
(b) Reinforcement Learning
(c) Facial Expression Recognition
(d) Recommendation System Algorithms
56.  Virtual face filters must maintain consistent performance under varying environmental conditions. Which technical challenge is most significant when a user moves from brightly lit to dimly lit environments?
(a) Degradation of network connectivity.
(b) Increased complexity in audio processing.
(c) Reduced accuracy of facial landmark detection and feature tracking.
(d) Higher power consumption of the device.
57.  How do robust virtual face filter systems typically handle the diversity of human face shapes, sizes, and proportions to apply filters effectively across different users?
(a) By using a single, fixed filter overlay that stretches automatically.
(b) By requiring users to manually adjust the filter's size and position.
(c) By employing statistical shape models (e.g., Active Appearance Models) to adapt filter placement.
(d) By collecting and storing a pre-trained model for every possible face type.
58.  The fundamental difference in how a 2D virtual face filter (like a sticker overlay) and a 3D virtual face filter (like a virtual mask) interact with the user's face largely depends on:
(a) The processing power of the device.
(b) The amount of data transmitted to the cloud.
(c) The dimensionality of the facial model and transformations used.
(d) The color palette of the filter assets.
59.  For highly realistic 3D virtual face filters that deform or blend with the user's face (e.g., changing facial structure), what additional input or estimation is often crucial beyond 2D facial landmarks?
(a) User's geographical location data.
(b) User's current heart rate.
(c) Depth information or 3D mesh reconstruction of the face.
(d) Ambient temperature readings.
60.  If a virtual face filter application experiences repeated, brief failures in accurately detecting facial landmarks, what is the most likely immediate impact on the user experience?
(a) The application will crash unexpectedly.
(b) The filter will appear to jump, flicker, or momentarily disappear from the face.
(c) The device's battery will drain rapidly.
(d) The application will automatically switch to a different filter.


